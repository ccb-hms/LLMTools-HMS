{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9c1d4c-45e2-48e3-93cf-9c61e46518da",
   "metadata": {},
   "source": [
    "### \"Hello World\" example to connect to a local Ollama model running in a container ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30941ddf-a4b5-4ba4-8c4b-7a9ddd023bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Package version: v0.0.1\n",
      "Authors: The Core for Computational Biomedicine at Harvard Medical School\n",
      "https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ollama\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import this module with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import llmtools\n",
    "from llmtools.ollamamodel import Ollama as OllamaModel\n",
    "\n",
    "print(f'Package version: {llmtools.__version__}')\n",
    "print(f'Authors: {llmtools.__authors__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b06ebeb-54a5-4e2c-9f55-3fb2d3e8122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /app/data\n"
     ]
    }
   ],
   "source": [
    "# Directories and files\n",
    "data_dir = os.environ.get('DATA')\n",
    "print(f'Data directory: {data_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29e882e-1dc4-4fb1-8517-3a591070f64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemma3:latest']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemma3:latest: 100%|██████████| 489/489 [00:00<00:00, 2.56kB/s, success]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Let's check which models we have available\n",
    "print(OllamaModel().list_models())\n",
    "\n",
    "# Pull a model for demonstration\n",
    "success = OllamaModel().pull_model(model_name='gemma3:latest')\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac0ed293-f3fd-448a-9b43-01ff53bdd6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'system', 'content': 'You are a powerful AI system.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': 'Explain in a short paragraph how a transformer-based language model works.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Okay, here’s a breakdown of how transformer-based language models work, in a nutshell:\n",
      "\n",
      "At their core, transformers rely on a mechanism called “attention.” Instead of processing words sequentially like older models, they look at *all* words in a sentence simultaneously. This “attention” allows the model to understand the relationships between words, regardless of their distance. The model learns to assign weights to these relationships, indicating how important each word is to the context of others. It then uses these weights to predict the next word in a sequence, constantly refining its understanding of the text as it goes. Essentially, they’re exceptionally good at capturing long-range dependencies within language. \n",
      "\n",
      "---\n",
      "\n",
      "Do you want me to delve deeper into a specific aspect, like attention mechanisms, training, or applications?\n"
     ]
    }
   ],
   "source": [
    "# Run a simple prompt\n",
    "system_prompt = 'You are a powerful AI system.'\n",
    "user_prompt = 'Explain in a short paragraph how a transformer-based language model works.'\n",
    "\n",
    "# Package messages\n",
    "messages = OllamaModel.create_messages(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "display(*messages)\n",
    "\n",
    "# Send messages to model\n",
    "model_name = 'gemma3:latest'\n",
    "temperature = 0.7\n",
    "client = OllamaModel().create_client()\n",
    "response = client.chat(model=model_name,\n",
    "                       messages=messages,\n",
    "                       options={'temperature': temperature})\n",
    "print()\n",
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
