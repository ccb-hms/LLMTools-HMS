{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844c89eb",
   "metadata": {},
   "source": [
    "# Excercise: MedMCQA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54851631",
   "metadata": {},
   "source": [
    "*MedMCQA dataset description*\n",
    "\n",
    "MedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions.\n",
    "\n",
    "MedMCQA has more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity.\n",
    "\n",
    "Each sample contains a question, correct answer(s), and other options which require a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects, including the followings:\n",
    "Anesthesia, Anatomy, Biochemistry, Dental, ENT, Forensic Medicine (FM), Obstetrics and Gynecology (O&G), Medicine, Microbiology, Ophthalmology, Orthopedics, Pathology, Pediatrics, Pharmacology, Physiology, Psychiatry, Radiology, Skin, Preventive & Social Medicine (PSM), Surgery.\n",
    "\n",
    "More information in Reference: https://huggingface.co/datasets/openlifescienceai/medmcqa\n",
    "\n",
    "*Exercise Description*\n",
    "\n",
    "**Objective**: Use gemma3:12b (or any other model from Ollama) to answer questions from the MedMCQA dataset, based on the following guidelines.\n",
    "- **Subjects to Include**: Focus on questions related to the following subjects: Dental, Pathology, Surgery, Medicine, Anaesthesia, and Radiology.\n",
    "- **Scope**: limited to validation dataset.\n",
    "- **Evaluation Metrics**: \n",
    "1. Overall accuracy.\n",
    "2. Accuracy per subject.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba0bcd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3978d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import ollama\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from ollama import chat\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84041bfd",
   "metadata": {},
   "source": [
    "## Getting Started: Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama list  --- check list of models available \n",
    "# !ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da03787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pull gemma3:12b if it does not exist\n",
    "# !ollama pull gemma3:12b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Prompt/User input\n",
    "query = 'Tell me a joke'\n",
    "message = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "  {\"role\": \"user\", \"content\": f\"{query}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b23f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submit your request\n",
    "model_id = 'gemma3:12b'\n",
    "response = chat(model=model_id, messages=message)\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918ffdb",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ad14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "# https://huggingface.co/datasets/openlifescienceai/medmcqa\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "\n",
    "data= load_dataset(\"openlifescienceai/medmcqa\")\n",
    "\n",
    "## to preview the data\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert validation dataset to dataframe\n",
    "data = data['validation'].to_pandas()\n",
    "data.head()\n",
    "\n",
    "## print data shape\n",
    "pprint.pprint(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79431bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## show questions types\n",
    "pprint.pprint(data['choice_type'].value_counts())\n",
    "\n",
    "\n",
    "## show count of subjects\n",
    "pprint.pprint(data['subject_name'].value_counts())\n",
    "\n",
    "\n",
    "## Filter the dataset by:\n",
    "# Single questions\n",
    "data = data[data.choice_type =='single']\n",
    "\n",
    "\n",
    "# Subject_names : [ 'Dental', 'Pathology','Surgery', 'Medicine', 'Anaesthesia', 'Radiology']\n",
    "subject_names = [ 'Dental', 'Pathology','Surgery', 'Medicine', 'Anaesthesia', 'Radiology']\n",
    "data = data[data.subject_name.isin(subject_names)]\n",
    "\n",
    "\n",
    "# Show final data shape\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.shape\n",
    "pprint.pprint(data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c14c07",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions to prapare data for inference\n",
    "\n",
    "def generate_message(query):\n",
    "\n",
    "    \"\"\"\n",
    "    Constructs a message for an AI assistant to answer a medical multiple-choice question.\n",
    "\n",
    "    The function returns a list of messages formatted for use with chat-based language models\n",
    "    (e.g., OpenAI's GPT API), setting the assistant's role and providing context with an example.\n",
    "\n",
    "    The assistant is instructed to act as a medical doctor and select the correct answer from \n",
    "    the given choices (A to D) without explanation.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The medical multiple-choice question formatted with options.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the chat message history. It includes:\n",
    "            - A system message with detailed instructions and an example format.\n",
    "            - A user message containing the input query.\n",
    "    \"\"\"\n",
    "    instruction = '''You are a medical doctor answering real-world medical entrance exam questions. Based on your understanding of basic and clinical science, medical knowledge, and mechanisms underlying health, disease, patient care, and modes of therapy, answer the following multiple-choice question. Select one correct answer from A to D. Do not provide reasons. Only the letter of the correct answer. Follow the example below.'''\n",
    "\n",
    "    example = '''Question: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma:\n",
    "    (A) Hyperplasia\n",
    "    (B) Hyperophy\n",
    "    (C) Atrophy\n",
    "    (D) Dyplasia\n",
    "    Answer: C'''\n",
    "\n",
    "\n",
    "    message = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are a helpful AI assistant. {instruction} \\n\\n Example:\\n\\n{example}\"}, \n",
    "    {\"role\": \"user\", \"content\": f\"{query}\"}\n",
    "    ]\n",
    "    return message\n",
    "\n",
    "def create_query(x):\n",
    "\n",
    "    \"\"\"\n",
    "    Formats a dictionary containing a medical question and answer choices into a structured prompt.\n",
    "\n",
    "    The function accepts a dictionary with a question and four options, and returns a string \n",
    "    formatted as a multiple-choice question, suitable for passing to a language model.\n",
    "\n",
    "    Parameters:\n",
    "        x (dict): A dictionary with the following keys:\n",
    "            - 'question' (str): The question text.\n",
    "            - 'opa' (str): Option A.\n",
    "            - 'opb' (str): Option B.\n",
    "            - 'opc' (str): Option C.\n",
    "            - 'opd' (str): Option D.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted multiple-choice question string with options labeled (A) to (D).\n",
    "    \"\"\"\n",
    "\n",
    "    question = x['question']\n",
    "    option_A = x['opa']\n",
    "    option_B = x['opb']\n",
    "    option_C = x['opc']\n",
    "    option_D =  x['opd']\n",
    "    prompt = f'''Question: {question}\n",
    "    (A) {option_A}\n",
    "    (B) {option_B}\n",
    "    (C) {option_C}\n",
    "    (D) {option_D}\n",
    "    '''\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d92678",
   "metadata": {},
   "source": [
    "##  Example (1 sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Step 1. Create query\n",
    "example = data.iloc[0] ## first sample/row/question\n",
    "query = create_query(example)\n",
    "## show the query\n",
    "pprint.pprint(query)\n",
    "\n",
    "## Step 2. Generate message\n",
    "message  = generate_message(query)\n",
    "## show the message\n",
    "pprint.pprint(message)\n",
    "\n",
    "\n",
    "## Step 3. Submit request\n",
    "response = chat(model=model_id, messages=message)\n",
    "\n",
    "## show the model answer\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f115ac0",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aea9cb-8cb9-40ae-b090-03d9b4591322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1. Create queries\n",
    "data['query'] = data.apply(create_query,axis=1)\n",
    "\n",
    "## Step 2. Generate messages\n",
    "queries = data['query'].values\n",
    "messages = [generate_message(query) for query in queries]\n",
    "\n",
    "## Step 3. Submit requests and save results\n",
    "def get_ollama_response(message, model_name = model_id):\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=message\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor: # Adjust max_workers as needed\n",
    "    outputs = list(executor.map(get_ollama_response, messages))\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Execution time: {end_time - start_time:.4f} seconds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28386806-4476-478a-8187-5739ad90e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step 4 save results\n",
    "responses = {}\n",
    "for id_, response in zip( data['id'].values,outputs):\n",
    "    responses[id_] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "# Save data to a JSON file\n",
    "with open(\"o2-demo-LLMresponses.json\", \"w\") as json_file:\n",
    "    json.dump(responses, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f5411",
   "metadata": {},
   "source": [
    "# Response processing and performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response processing\n",
    "\n",
    "# Parse the model answer.\n",
    "def extract_answer(x):\n",
    "    x = x.lower()\n",
    "    x = x.replace('answer:', '')\n",
    "    return x.strip()\n",
    "\n",
    "parsed_answers= {}\n",
    "for id_ in responses:\n",
    "    parsed_answers[id_] =extract_answer(responses[id_])\n",
    "data[f'pred_{model_id}'] = data['id'].map(parsed_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7fdeb4",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a042420",
   "metadata": {},
   "outputs": [],
   "source": [
    "## overall accuracy\n",
    "def  compute_accuracy(y_pred, y_true):\n",
    "    count=0\n",
    "    for y, yi in zip(y_pred, y_true):\n",
    "        if y==yi:\n",
    "            count +=1\n",
    "    return count/len(y_true)\n",
    "\n",
    "## map prediction to 0,1,2,4\n",
    "labels2ids_dict= {'a': int(0), 'b': int(1), 'c': int(2), 'd': int(3)}\n",
    "data[f'pred_{model_id}'] = data[f'pred_{model_id}'].map(labels2ids_dict)\n",
    "\n",
    "# Step 7. Compute overall accuracy\n",
    "\n",
    "y_true = data['cop'] ### answer of the question, as per the dataset\n",
    "y_pred = data[f'pred_{model_id}'] \n",
    "overall_accuracy = compute_accuracy(y_pred, y_true)\n",
    "print (f'Overall accuracy of {model_id}: {overall_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1721412",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Field Accuracy\n",
    "subject_names = [ 'Dental', 'Pathology','Surgery', 'Medicine', 'Anaesthesia', 'Radiology'] \n",
    "subject_accuracy = {}\n",
    "for subject in subject_names:\n",
    "    X = data[data['subject_name']== subject]\n",
    "    y_true = X['cop']\n",
    "    y_pred = X[f'pred_{model_id}']\n",
    "    subject_accuracy[subject] = compute_accuracy(y_true, y_pred)\n",
    "df_subjectAccuracies = pd.DataFrame( {model_id: subject_accuracy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8491b",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "## show performance\n",
    "df_subjectAccuracies['Subject'] = df_subjectAccuracies.index\n",
    "print (df_subjectAccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart\n",
    "ax = df_subjectAccuracies.plot(kind='bar')  # each column is hue, each index is category\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Subjects\")\n",
    "plt.title(\"Performance\")\n",
    "plt.xticks(rotation= 'horizontal')\n",
    "plt.tight_layout()\n",
    "# Remove all spines (frames)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.legend(title=\"Models\", bbox_to_anchor=(0.5, -0.15), loc='upper center', ncol=2)  # Adjust legend position\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4be482-42b3-4d0c-998d-232bd9f2cec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
